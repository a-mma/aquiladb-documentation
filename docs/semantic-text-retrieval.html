<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Semantic Text Retrieval · AquilaDB</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Efficient Semantic Text Retrieval with Google&#x27;s Universal Sentence Encoder and AquilaDB"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Semantic Text Retrieval · AquilaDB"/><meta property="og:type" content="website"/><meta property="og:url" content="https://aquiladb.xyz/"/><meta property="og:description" content="# Efficient Semantic Text Retrieval with Google&#x27;s Universal Sentence Encoder and AquilaDB"/><meta property="og:image" content="https://aquiladb.xyz/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://aquiladb.xyz/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon_io/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-156570061-1"></script><script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments); }
              gtag('js', new Date());
              gtag('config', 'UA-156570061-1');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&amp;display=swap"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js" async=""></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon_io/favicon.ico" alt="AquilaDB"/><h2 class="headerTitleWithLogo">AquilaDB</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a target="_self"></a></li><li class="siteNavGroupActive"><a href="/docs/introduction" target="_self">Docs</a></li><li class=""><a href="/docs/api-reference" target="_self">API</a></li><li class=""><a href="/help" target="_self">Help</a></li><li class=""><a href="https://medium.com/a-mma" target="_self">Blog</a></li><li class=""><a href="https://github.com/a-mma/AquilaDB" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Tutorials</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Introduction</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/introduction">Introduction</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">How-To Guides</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/install-aquiladb">Install AquilaDB</a></li><li class="navListItem"><a class="navItem" href="/docs/modify-default-configuration">Modify Default Configuration</a></li><li class="navListItem"><a class="navItem" href="/docs/run-aquiladb">Run AquilaDB</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/get-started">Get Started</a></li><li class="navListItem"><a class="navItem" href="/docs/hands-on-mnist">AquilaDB hands on with MNIST dataset</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/semantic-text-retrieval">Search text by its meaning</a></li><li class="navListItem"><a class="navItem" href="/docs/reverse-image-search">Build a toy Google reverse image search</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Core Concepts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/neural-information-retrieval">Neural Information retrieval</a></li><li class="navListItem"><a class="navItem" href="/docs/aquiladb-architecture">AquilaDB Architecture</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Reference</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/adb-benchmarks">Benchmarks</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/a-mma/aquiladb-documentation/tree/master/docs/semantic-text-retrieval.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Semantic Text Retrieval</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="efficient-semantic-text-retrieval-with-googles-universal-sentence-encoder-and-aquiladb"></a><a href="#efficient-semantic-text-retrieval-with-googles-universal-sentence-encoder-and-aquiladb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Efficient Semantic Text Retrieval with Google's Universal Sentence Encoder and AquilaDB</h1>
<p><em><strong>Download Jupyter notebook <a href="https://github.com/a-mma/AquilaDB-Examples/tree/master/QnA_with_USE">here</a></strong></em></p>
<p>In this tutorial, we are going to look at how AquilaDB vector database can help in efficient Semantic Retrieval with Google's Universal Sentence Encoder (USE).</p>
<p>This tutorial is following the same idea as described <a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">in latest post on Universal Sentence Encoder at Google AI blog</a>. We encourage you to read that blog before proceeding. Because it is very useful to get a context on what we are going to do below.</p>
<blockquote>
<p>One difference in this tutorial is that, we use <code>universal-sentence-encoder-large</code> which belongs to the same <code>USE</code> family for simplicity in explanation. The idea explained here is the very same for all models in <code>USE</code> family.</p>
</blockquote>
<p>This is an image taken from that blog post. A recommended pipeline for textual similarity. <code>AquilaDB</code> will cover <code>pre-encoded Candidates</code> data store and <code>ANN search</code> modules in this pipeline. Cool.. Right?</p>
<p><img src="https://1.bp.blogspot.com/-q1g13xLR-9E/XSi8ZewIXzI/AAAAAAAAETQ/Oek9K51ZrAQvbZL3t3rme5HcegzCNm98QCEwYBhgL/s640/image1.png" alt="A prototypical semantic retrieval pipeline, used for textual similarity"></p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Let's import required modules</span>

<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="load-pretrained-encoder"></a><a href="#load-pretrained-encoder" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load pretrained encoder</h3>
<p>We need to load pre-trained USE model from Tensorflow Hub. We use this model to encode our sentences before sending it to AquilaDB for indexing and querying.</p>
<pre><code class="hljs css language-python">use_module_url = <span class="hljs-string">"https://tfhub.dev/google/universal-sentence-encoder-large/3"</span>

<span class="hljs-comment"># load Universal Sentence Encoder module from tensor hub</span>
embed_module = hub.Module(use_module_url)
</code></pre>
<p>Let's test our loaded model with some random texts before proceeding.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># let's create some test sentanaces</span>
test_messages = [<span class="hljs-string">"AquilaDB is a Resillient, Replicated, Decentralized, Host neutral storage for Feature Vectors along with Document Metadata."</span>, 
            <span class="hljs-string">"Do k-NN retrieval from anywhere, even from the darkest rifts of Aquila (in progress). It is easy to setup and scales as the universe expands."</span>]
</code></pre>
<p>We feed our text array to model for embedding. Don't forget to wrap the embedding logic into a method to reuse it.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># helper function to generate embedding for input array of sentances</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_embeddings</span> <span class="hljs-params">(messages_in)</span>:</span>
    <span class="hljs-comment"># generate embeddings</span>
    <span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> session:
        session.run([tf.global_variables_initializer(), tf.tables_initializer()])
        message_embeddings = session.run(embed_module(messages_in))
        
    <span class="hljs-keyword">return</span> message_embeddings
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># print generated embeddings</span>
print(generate_embeddings(test_messages))
</code></pre>
<pre><code class="hljs">[[-0.00570544  0.01024008  0.04416275 ...  0.03282805 -0.01723128
   0.00956334]
 [ 0.0124177   0.09862255  0.06958324 ... -0.00700251  0.02332876
  -0.09377097]]
</code></pre>
<p>As you can see above, we were able to encode out random texts into corresponding sentance embedding with <code>USE</code>.</p>
<h3><a class="anchor" aria-hidden="true" id="lets-load-actual-data"></a><a href="#lets-load-actual-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Let's load actual data</h3>
<p>We will be loading some text from a <a href="https://github.com/a-mma/AquilaDB-Examples/blob/master/QnA_with_USE/article_set.txt?raw=true">text file</a>. This is a small wiki article set in plain text format.</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'article_set.txt'</span>, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> file_in:
    lines = file_in.readlines()
</code></pre>
<p>Let's write some helper functions. These functions will help us communicate with AquilaDB. You don't have to worry about this part now. Just keep it as is except for the IP address <code>192.168.1.100</code>. Replace that with the IP address where your AquilaDB installation is. Most probably, it is the same machine you are using now - then give <code>localhost</code> as address.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># helper functions to generate documents</span>

<span class="hljs-keyword">import</span> grpc

<span class="hljs-keyword">import</span> vecdb_pb2
<span class="hljs-keyword">import</span> vecdb_pb2_grpc

channel = grpc.insecure_channel(<span class="hljs-string">'192.168.1.100:50051'</span>)
stub = vecdb_pb2_grpc.VecdbServiceStub(channel)

<span class="hljs-comment"># API interface to add documents to AquilaDB</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">addDocuments</span> <span class="hljs-params">(documents_in)</span>:</span>
    response = stub.addDocuments(vecdb_pb2.addDocRequest(documents=documents_in))
    <span class="hljs-keyword">return</span> response


<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">import</span> json

<span class="hljs-comment"># helper function to convert native data to API friendly data</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convertDocuments</span><span class="hljs-params">(vector, document)</span>:</span>
    <span class="hljs-keyword">return</span> {
            <span class="hljs-string">"vector"</span>: {
                <span class="hljs-string">"e"</span>: vector
            },
            <span class="hljs-string">"b64data"</span>: json.dumps(document, separators=(<span class="hljs-string">','</span>, <span class="hljs-string">':'</span>)).encode(<span class="hljs-string">'utf-8'</span>)
        }


<span class="hljs-comment"># API interface to get nearest documents from AquilaDB</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getNearest</span> <span class="hljs-params">(matrix_in, k_in)</span>:</span>
    response = stub.getNearest(vecdb_pb2.getNearestRequest(matrix=matrix_in, k=k_in))
    <span class="hljs-keyword">return</span> response


<span class="hljs-comment"># helper function to convert native data to API friendly data</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convertMatrix</span><span class="hljs-params">(vector)</span>:</span>
    <span class="hljs-keyword">return</span> [{
            <span class="hljs-string">"e"</span>: vector
    }]
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="send-documents-to-aquiladb-for-indexing"></a><a href="#send-documents-to-aquiladb-for-indexing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Send documents to AquilaDB for indexing</h3>
<p>As mentioned previously, we need to store pre encoded candidates in a vector database to perform semantic similarity retrieval later. So, what we are going to do here is to take each line from wiki articles, encode them with <code>USE</code> model, attach the original wiki text with the resulting vector as metadata and send them to AquilaDB for indexing.</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> time

<span class="hljs-comment"># set a batch length</span>
batch_len = <span class="hljs-number">200</span>
<span class="hljs-comment"># counter to init batch sending of documents</span>
counter = <span class="hljs-number">0</span>
<span class="hljs-comment"># to keep generated documents</span>
docs_gen = []
<span class="hljs-comment"># to keep lines batch</span>
lbatch = []

<span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines:
    lbatch.append(line)
    <span class="hljs-keyword">if</span> len(lbatch) == batch_len:
        counter = counter + <span class="hljs-number">1</span>
        <span class="hljs-comment"># generate embeddings</span>
        vectors = generate_embeddings(lbatch)
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(vectors)):
            docs_gen.append(convertDocuments(vectors[i], {<span class="hljs-string">"text"</span>: lbatch[i]}))
        <span class="hljs-comment"># add documents to AquilaDB</span>
        response = addDocuments(docs_gen)
        print(<span class="hljs-string">"index: "</span>+str(counter), <span class="hljs-string">"inserted: "</span>+str(len(response._id)))
        docs_gen = []
        lbatch = []
</code></pre>
<pre><code class="hljs">index: 1 inserted: 199
index: 2 inserted: 178
...
index: 77 inserted: 186
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="query-the-database"></a><a href="#query-the-database" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Query the database</h3>
<p>Now, we need to retrieve semantically similar sentance to our input query from the database. It is straight forward. Just encode the query text with the same <code>USE</code> model and then perform k-NN query on the database.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Method to query for nearest neighbours</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">query_nn</span> <span class="hljs-params">(query)</span>:</span>
    query = [query]
    vector = generate_embeddings(query)[<span class="hljs-number">0</span>]
    
    converted_vector = convertMatrix(vector)
    nearest_docs_result = getNearest(converted_vector, <span class="hljs-number">1</span>)
    nearest_docs_result = json.loads(nearest_docs_result.documents)
    
    <span class="hljs-keyword">return</span> nearest_docs_result
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Let's try an example query.</span>

print(query_nn(<span class="hljs-string">'what are the subfamilies of duck'</span>)[<span class="hljs-number">0</span>][<span class="hljs-string">'doc'</span>][<span class="hljs-string">'text'</span>])
</code></pre>
<pre><code class="hljs">Swans are birds of the family Anatidae, which also includes geese and ducks. Swans are grouped with the closely related geese in the subfamily Anserinae where they form the tribe Cygnini. Sometimes, they are considered a distinct subfamily, Cygninae. Swans usually mate for life, though 'divorce' does sometimes occur, particularly following nesting failure. The number of eggs in each clutch ranges from three to eight.
</code></pre>
<p>That's all for this tutorial. Thanks, happy hacking..!</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/hands-on-mnist"><span class="arrow-prev">← </span><span class="function-name-prevnext">AquilaDB hands on with MNIST dataset</span></a><a class="docs-next button" href="/docs/reverse-image-search"><span>Build a toy Google reverse image search</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><h5>Docs</h5><a href="/docs/introduction">Getting Started</a><a href="/docs/get-started">Tutorials</a><a href="/docs/api-reference">API Reference</a></div><div><h5>Community</h5><a href="https://discord.gg/5YP7zHS">Project Chat</a></div><div><h5>More</h5><a href="https://medium.com/a-mma" target="_blank">Blog</a><a href="https://github.com/a-mma/AquilaDB" target="_blank">GitHub</a><a class="github-button" href="https://github.com/a-mma/AquilaDB" data-icon="octicon-star" data-count-href="/AquilaDB/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://github.com/a-mma/AquilaDB" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/amma.png" alt="a-mma" width="100" height="45"/></a><section class="copyright">Copyright © 2020 a-mma</section></footer></div></body></html>