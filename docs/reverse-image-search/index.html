<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Perform Direct and Reverse image search like Google Images · AquilaDB</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Perform Direct and Reverse image search like Google Images"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Perform Direct and Reverse image search like Google Images · AquilaDB"/><meta property="og:type" content="website"/><meta property="og:url" content="https://aquiladb.xyz/"/><meta property="og:description" content="# Perform Direct and Reverse image search like Google Images"/><meta property="og:image" content="https://aquiladb.xyz/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://aquiladb.xyz/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/img/favicon_io/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto&amp;display=swap"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js" async=""></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/favicon_io/favicon.ico" alt="AquilaDB"/><h2 class="headerTitleWithLogo">AquilaDB</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a target="_self"></a></li><li class="siteNavGroupActive"><a href="/docs/introduction" target="_self">Docs</a></li><li class=""><a href="/docs/api-reference" target="_self">API</a></li><li class=""><a href="/help" target="_self">Help</a></li><li class=""><a href="https://medium.com/a-mma" target="_self">Blog</a></li><li class=""><a href="https://github.com/a-mma/AquilaDB" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Tutorials</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Introduction</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/introduction">Introduction</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">How-To Guides</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/install-aquiladb">Install AquilaDB</a></li><li class="navListItem"><a class="navItem" href="/docs/modify-default-configuration">Modify Default Configuration</a></li><li class="navListItem"><a class="navItem" href="/docs/run-aquiladb">Run AquilaDB</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Tutorials</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/get-started">Get Started</a></li><li class="navListItem"><a class="navItem" href="/docs/hands-on-mnist">AquilaDB hands on with MNIST dataset</a></li><li class="navListItem"><a class="navItem" href="/docs/semantic-text-retrieval">Search text by its meaning</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/reverse-image-search">Build a toy Google reverse image search</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Core Concepts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/neural-information-retrieval">Neural Information retrieval</a></li><li class="navListItem"><a class="navItem" href="/docs/aquiladb-architecture">AquilaDB Architecture</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Reference</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/adb-benchmarks">Benchmarks</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><a class="edit-page-link button" href="https://github.com/a-mma/aquiladb-documentation/tree/master/docs/reverse-image-search.md" target="_blank" rel="noreferrer noopener">Edit</a><h1 class="postHeaderTitle">Perform Direct and Reverse image search like Google Images</h1></header><article><div><span><h1><a class="anchor" aria-hidden="true" id="perform-direct-and-reverse-image-search-like-google-images"></a><a href="#perform-direct-and-reverse-image-search-like-google-images" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Perform Direct and Reverse image search like Google Images</h1>
<p><em><strong>Download Jupyter notebook <a href="https://github.com/a-mma/AquilaDB-Examples/tree/master/2way_image_search">here</a></strong></em></p>
<p>In this tutorial, we will be looking at how multi-model search can be done in AquilaDB. We will build a tool similar to Google Image search and we will be performing direct (text to image) and reverse (image to image) search with the help of two pretrained models - one is for text and the other one for image.</p>
<p>To make things faster and easier, will be using a <code>Fasttext</code> model for sentence embedding and a <code>MobileNet</code> model for image encoding.</p>
<p>This tutorial will be fast and will skim some unwanted details in code. If you find it hard to follow, please refer to previous tutorials where we take more time to discuss those details in the code.</p>
<p>So, Let's begin..</p>
<h3><a class="anchor" aria-hidden="true" id="prerequisites"></a><a href="#prerequisites" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prerequisites</h3>
<p>Install and import all required python libraries (we will be installing &amp; importing AquilaDb library later).</p>
<pre><code class="hljs css language-python">!pip install fasttext
!pip install Pillow
!pip install matplotlib
!pip install <span class="hljs-string">"tensorflow_hub==0.4.0"</span>

<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub

<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> fasttext <span class="hljs-keyword">as</span> ft
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="load-flickr30k-images-dataset"></a><a href="#load-flickr30k-images-dataset" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load Flickr30k images dataset</h3>
<p>You need to download <a href="https://www.kaggle.com/hsankesara/flickr-image-dataset">Flickr Image captioning dataset</a> and extract it to a convenient location. We have extracted it into a directory <code>./flickr30k_images/</code> which is in the same directory as this notebook.</p>
<p>Load results.csv file as a Pandas dataframe - which contains all the captions along with file names of each image.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># read image descriptions</span>
image_descriptions = pd.read_csv(<span class="hljs-string">'./flickr30k_images/results.csv'</span>, sep=<span class="hljs-string">'\|\s'</span>, engine=<span class="hljs-string">'python'</span>)
selected_columns = [<span class="hljs-string">'image_name'</span>, <span class="hljs-string">'comment'</span>]
image_descriptions = image_descriptions[selected_columns]
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="train-and-load-fasttext-model"></a><a href="#train-and-load-fasttext-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Train and load Fasttext Model</h3>
<p>Now let's quickly build a Fasttext language model from the raw comments that we have.</p>
<p>To make things easy, we already have extracted all the comments from the CSV file to a text file - <code>results.txt</code>.
Let's train the Fasttext model on our data in skip-gram unsupervised mode.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># create a language model quickly with fasttext</span>
fasttext_model = ft.train_unsupervised(model=<span class="hljs-string">'skipgram'</span>, input=<span class="hljs-string">'flickr30k_images/results.txt'</span>)
<span class="hljs-comment"># save model</span>
fasttext_model.save_model(<span class="hljs-string">"ftxt_model.bin"</span>)
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># load saved model</span>
fasttext_model = ft.load_model(<span class="hljs-string">"ftxt_model.bin"</span>)
</code></pre>
<p>​</p>
<p>Verify that the model encodes the semantic information for different words properly.</p>
<p>Note that, fasttext is not good for encoding semantic information for sentences. We are using it here, because we expect the user to search images by giving importance to the words - resulting each object in the image rather than the overall context of the image.</p>
<p>In case you wanted semantic sentence based retrieval, feel free to use better language models (slower than Fasttext) like Universal Sentence Encoder. We have a tutorial on that <a href="https://github.com/a-mma/AquilaDB/wiki/Semantic-Text-Retrieval">over here</a>.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># test the language model</span>
! echo <span class="hljs-string">"girl"</span> | fasttext nn ftxt_model.bin
! echo <span class="hljs-string">"==============="</span> 
! echo <span class="hljs-string">"garden"</span> | fasttext nn ftxt_model.bin
! echo <span class="hljs-string">"==============="</span> 
! echo <span class="hljs-string">"glass"</span> | fasttext nn ftxt_model.bin
! echo <span class="hljs-string">"==============="</span> 
! echo <span class="hljs-string">"ball"</span> | fasttext nn ftxt_model.bin
</code></pre>
<pre><code class="hljs">Pre-computing word vectors... done.
Query word? little 0.81607
child 0.749877
Girl 0.730085
pink 0.729028
Little 0.728659
boy 0.721146
young 0.70541
Child 0.696403
blond 0.69241
pigtails 0.683836
...
Query word? ===============
Pre-computing word vectors... done.
Query word? t-ball 0.850855
T-ball 0.842449
ballgame 0.822507
A&amp;M 0.745541
Tennis 0.731484
Rugby 0.726965
rugby 0.719968
33 0.719124
defends 0.716951
racquet 0.71034
Query word? 
</code></pre>
<p>Just in case you wonder how we generate sentence embedding from Fasttext, here's a one-liner to do that.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># convert string to embeddings</span>
fasttext_model.get_sentence_vector(<span class="hljs-string">'a cat is sitting on the carpet'</span>)
</code></pre>
<pre><code class="hljs">array([ 3.55252177e-02,  4.62995056e-04, -5.44314571e-02, -3.67470682e-02,
        5.60869165e-02, -8.12834278e-02,  3.80968209e-03, -2.74911691e-02,
        ...
        5.96124977e-02, -1.29236341e-01,  5.84035628e-02,  1.21095881e-01,
        5.16762286e-02,  1.02854759e-01, -1.47027825e-03, -1.08863831e-01],
      dtype=float32)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="cleanup-data-dataframe"></a><a href="#cleanup-data-dataframe" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>cleanup data (dataframe)</h3>
<p>Before we proceed into the core of this tutorial, we need to cleanup the dataframe to keep only what we wanted. The code below is self explanatory, if you have a background knowledge using Pandas. We are skipping the explanation just because it is out of scope of this tutorial.</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">concater</span><span class="hljs-params">(x)</span>:</span>
    <span class="hljs-keyword">try</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-string">' '</span>.join(x)
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">return</span> <span class="hljs-string">''</span>

<span class="hljs-comment"># concatenate strings for same images</span>
image_descriptions[<span class="hljs-string">'comment'</span>] = image_descriptions.groupby([<span class="hljs-string">'image_name'</span>])[<span class="hljs-string">'comment'</span>].transform(concater)
image_descriptions = image_descriptions[[<span class="hljs-string">'image_name'</span>,<span class="hljs-string">'comment'</span>]].drop_duplicates()
image_descriptions.head(<span class="hljs-number">4</span>)
</code></pre>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>image_name</th>
      <th>comment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1000092795.jpg</td>
      <td>Two young guys with shaggy hair look at their ...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>10002456.jpg</td>
      <td>Several men in hard hats are operating a giant...</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1000268201.jpg</td>
      <td>A child in a pink dress is climbing up a set o...</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1000344755.jpg</td>
      <td>Someone in a blue shirt and hat is standing on...</td>
    </tr>
  </tbody>
</table>
<pre><code class="hljs css language-python"><span class="hljs-comment"># verify comments in each row</span>
print(image_descriptions.iloc[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], image_descriptions.iloc[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>])
print(image_descriptions.iloc[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>], image_descriptions.iloc[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>])
print(image_descriptions.iloc[<span class="hljs-number">500</span>][<span class="hljs-number">0</span>], image_descriptions.iloc[<span class="hljs-number">500</span>][<span class="hljs-number">1</span>])
</code></pre>
<pre><code class="hljs">1000092795.jpg Two young guys with shaggy hair look at their hands while hanging out in the yard . Two young , White males are outside near many bushes . Two men in green shirts are standing in a yard . A man in a blue shirt standing in a garden . Two friends enjoy time spent together .

10002456.jpg Several men in hard hats are operating a giant pulley system . Workers look down from up above on a piece of equipment . Two men working on a machine wearing hard hats . Four men on top of a tall structure . Three men on a large rig .

1159425410.jpg A female washes her medium-sized dog outdoors in a plastic container while a friend secures it with a leash . A brown dog is in a blue tub , while one person holds his leash and another is soaping him . Two people give a dog a bath outdoors in a blue container . A small brown dog is being washed in a small blue bin . A dog calmly waits until his bath is over .
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="load-pretrained-mobilenet-model"></a><a href="#load-pretrained-mobilenet-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Load pretrained MobileNet Model</h3>
<p>Now we need to load pretrained MobileNet model from Tensorflow Hub.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># load mobilenet featurevector model as a Keras layer</span>
module = tf.keras.Sequential([
    hub.KerasLayer(<span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"</span>, 
        output_shape=[<span class="hljs-number">1280</span>],
        trainable=<span class="hljs-literal">False</span>)
])

<span class="hljs-comment"># build the model</span>
module.build([<span class="hljs-literal">None</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>])

<span class="hljs-comment"># This model will only accept images of size 224 x 224</span>
<span class="hljs-comment"># So, we need to make sure throughout the code, that we supply correcty resized images</span>
im_height, im_width = <span class="hljs-number">224</span>, <span class="hljs-number">224</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="helper-functions"></a><a href="#helper-functions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Helper functions</h3>
<p>Here are some self explanatory helper functions that will help us during the embed/encode/predict stages.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Here is the helper function to load and resize image</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_rsize_image</span><span class="hljs-params">(filename, w, h)</span>:</span>
    <span class="hljs-comment"># open the image file</span>
    im = Image.open(filename)
    <span class="hljs-comment"># resize the image</span>
    im = im.resize(size=(w, h))
    <span class="hljs-keyword">return</span> np.asarray(im)
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># Let's test loading an image</span>
image_array = load_rsize_image(<span class="hljs-string">'./flickr30k_images/flickr30k_images/301246.jpg'</span>, im_width, im_height)
plt.imshow(image_array)
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862203-095b6380-bd23-11e9-80b2-96ba622ec514.png" alt="png"></p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># helper function to retrieve fasttext word embeddings</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_ftxt_embeddings</span><span class="hljs-params">(text)</span>:</span>
    <span class="hljs-keyword">return</span> fasttext_model.get_sentence_vector(text)

<span class="hljs-comment"># helper function to encode images with mobilenet</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_image_encodings</span><span class="hljs-params">(batch, module)</span>:</span>
    message_embeddings = module.predict(batch)
    <span class="hljs-keyword">return</span> message_embeddings
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># helper function to embed images and comments in a dataframe and return numpy matrices</span>
<span class="hljs-comment"># this function will iterate through a dataframe, which contains image file names in one column and</span>
<span class="hljs-comment"># comments in another column and will generate separate matrices for images and comments.</span>
<span class="hljs-comment"># row order of these matrices matters because same row index in both matrices represent related image and comments.</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">embed_all</span><span class="hljs-params">(df, w, h)</span>:</span>
    img_arr = []
    txt_arr = []
    <span class="hljs-comment"># for each row, embed data</span>
    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> df.iterrows():
        <span class="hljs-comment"># img_arr will contain all the image file data (will be passed to mobilenet later)</span>
        img_arr.append(load_rsize_image(<span class="hljs-string">'./flickr30k_images/flickr30k_images/'</span> + row[<span class="hljs-string">'image_name'</span>], w, h))
        <span class="hljs-comment"># txt_arr will contain all Fasttext sentance embedding for each comment </span>
        txt_arr.append(get_ftxt_embeddings(row[<span class="hljs-string">'comment'</span>]))
    <span class="hljs-keyword">return</span> img_arr, txt_arr
</code></pre>
<pre><code class="hljs css language-python">img_emb, txt_emb = embed_all(image_descriptions, im_width, im_height)
<span class="hljs-comment"># reset fasttext model</span>
fasttext_model = <span class="hljs-literal">None</span>
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># verify that image is image loded correctly</span>
plt.imshow(img_emb[<span class="hljs-number">2</span>])
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862204-09f3fa00-bd23-11e9-88d6-d8ff52ffec20.png" alt="png"></p>
<p>In above steps, we have embedded text data with Fasttext. Image data still need to be encoded. To keep the CPU and RAM away from exploding, we decided to do it in batches, before sending them to AquilaDB.</p>
<p>But just in case you wonder how an image can be encoded, here is a one-liner for that:</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># test image encodings generation</span>
get_image_encodings(np.true_divide(np.array(img_emb[<span class="hljs-number">0</span>:<span class="hljs-number">100</span>]), <span class="hljs-number">255</span>), module).shape
</code></pre>
<pre><code class="hljs">(100, 1280)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="filter-based-indexing"></a><a href="#filter-based-indexing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Filter based indexing</h3>
<p>This is the core idea we wanted to share with you through this tutorial.
In this tutorial, we are using multiple models that generate encodings. So we need to index both of them inside AquilaDB and need to somehow discriminate (filter) them during k-NN search. With AquilaDB we could do this efficiently.</p>
<p>Padding can be done in two ways:</p>
<ol>
<li>Positional padding</li>
<li>Filter vector padding</li>
</ol>
<h4><a class="anchor" aria-hidden="true" id="positional-padding"></a><a href="#positional-padding" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Positional Padding</h4>
<p>This is what we will be doing in this tutorial.
If you have a limited number of models ranging between 2 to 4, this will be the best method that you can use.</p>
<p>Suppose, we have two models <code>M1</code> and <code>M2</code>. And these models generate vectors <code>v1</code> and <code>v2</code>.
Then we will build two long vectors <code>vlong</code> as, <code>size(vlong) = size(v1) + size(v2)</code> for each models.</p>
<p>Then we will pad each of them with either preceding or following zeroes.</p>
<p>Example:</p>
<p>v1 = [1, 2, 3, 4, 5]</p>
<p>v2 = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]</p>
<p>then; size(vlong) = 5 + 10 = 15</p>
<p>So, we will be sending two vectors to AquilaDB, each of them are:</p>
<p>v1long = [1, 2, 3, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</p>
<p>v2long = [0, 0, 0, 0, 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]</p>
<h4><a class="anchor" aria-hidden="true" id="filter-vector-padding"></a><a href="#filter-vector-padding" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Filter vector padding</h4>
<p>If you have more than 4 models, we highly recommend you to use a better Machine Learning model that combine all of these and then use <code>Positional Padding</code>. But, of course there might be requirements apart from that, then use this method.</p>
<p>Consider designing filter vectors for each model. For example, we have two models M1 and M2. And these models generate vectors v1 and v2. Then, design two filter vectors f1 and f2 as,</p>
<p>f1 = [0, 0, 0, 0, 0, 0, ........ n items]</p>
<p>f1 = [1, 1, 1, 1, 1, 1, ........ n items]</p>
<p>value of <code>n</code> is a variable should be chosen to maximize the distance between two filters.</p>
<p>So, we will be sending two vectors to AquilaDB, each of them are:</p>
<p>v1long = append(f1, v1)</p>
<p>v2long = append(f2, v2)</p>
<h3><a class="anchor" aria-hidden="true" id="send-data-to-aquiladb-for-indexing"></a><a href="#send-data-to-aquiladb-for-indexing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Send data to AquilaDB for indexing</h3>
<pre><code class="hljs css language-python"><span class="hljs-comment"># install AquilaDb python client</span>

! pip install aquiladb

<span class="hljs-comment"># import AquilaDB client</span>
<span class="hljs-keyword">from</span> aquiladb <span class="hljs-keyword">import</span> AquilaClient <span class="hljs-keyword">as</span> acl
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># create DB instance.</span>
<span class="hljs-comment"># Please provide the IP address of the machine that have AquilaDB installed in.</span>
db = acl(<span class="hljs-string">'192.168.1.102'</span>, <span class="hljs-number">50051</span>)

<span class="hljs-comment"># let's get our hands dirty for a moment..</span>
<span class="hljs-comment"># convert a sample dirty Document</span>
sample = db.convertDocument([<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>,<span class="hljs-number">0.3</span>,<span class="hljs-number">0.4</span>], {<span class="hljs-string">"hello"</span>: <span class="hljs-string">"world"</span>})
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-comment"># and print it</span>
sample
</code></pre>
<pre><code class="hljs">{'vector': {'e': [0.1, 0.2, 0.3, 0.4]}, 'b64data': b'{&quot;hello&quot;:&quot;world&quot;}'}
</code></pre>
<p>As you can see above, this is what happens when a document along with a vector is serialized. This will then be sent to AquilaDB.</p>
<h5><a class="anchor" aria-hidden="true" id="add-documents-to-aquiladb"></a><a href="#add-documents-to-aquiladb" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>add documents to AquilaDB</h5>
<p>In the code below we do a lot of things. So, please pay attention to the comments to see how it is done.</p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># We are going to encode a small portion (6000) images/text that we have downloaded.</span>
<span class="hljs-comment"># You can add more if you have got enough interest, patience and a good machine.</span>

<span class="hljs-comment"># batch length - to be sent to mobilenet for encoding</span>
blen = <span class="hljs-number">500</span>
<span class="hljs-comment"># which index to start encoding - ofcause its 0</span>
vstart = <span class="hljs-number">0</span>
<span class="hljs-comment"># How much images/text we need to encode</span>
vend = <span class="hljs-number">6000</span>

<span class="hljs-comment"># convert text embeddings to numpy array</span>
txt_emb = np.array(txt_emb)

<span class="hljs-comment"># iterate over each batch of image/text data/embedding</span>
<span class="hljs-keyword">for</span> ndx <span class="hljs-keyword">in</span> range(vstart, vend, blen):
    <span class="hljs-comment"># encode each batch of images</span>
    image_encoding = get_image_encodings(np.true_divide(np.array(img_emb[ndx:ndx+blen]), <span class="hljs-number">255</span>), module)
    
    <span class="hljs-comment"># pad image and text vectors - this is discussed in section `filter based indexing`</span>
    <span class="hljs-comment"># select subset of data we're interested for text embeddings</span>
    text_embedding = txt_emb[ndx:ndx+blen]
    <span class="hljs-comment"># pad text encodings with trailing zeros</span>
    text_embedding = np.pad(text_embedding, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1280</span>)), <span class="hljs-string">'constant'</span>)
    <span class="hljs-comment"># pad image encodings with preceding zeros</span>
    image_encoding = np.pad(image_encoding, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">100</span>, <span class="hljs-number">0</span>)), <span class="hljs-string">'constant'</span>)
    
    <span class="hljs-comment"># finally, create and send each document</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(blen):
        <span class="hljs-comment"># create document - text</span>
        doc_txt = db.convertDocument(text_embedding[i], {<span class="hljs-string">"image_name"</span>: image_descriptions.iloc[ndx+i][<span class="hljs-number">0</span>]})
        <span class="hljs-comment"># create document - image</span>
        doc_img = db.convertDocument(image_encoding[i], {<span class="hljs-string">"image_name"</span>: image_descriptions.iloc[ndx+i][<span class="hljs-number">0</span>]})
        
        <span class="hljs-comment"># send documents - text</span>
        db.addDocuments([doc_txt])
        <span class="hljs-comment"># send documents - image</span>
        db.addDocuments([doc_img])
    
    <span class="hljs-comment"># Wooh! done with nth batch   </span>
    print(<span class="hljs-string">'Done: '</span>, ndx, ndx+blen)
</code></pre>
<pre><code class="hljs">Done:  0 500
Done:  500 1000
...
Done:  5500 6000
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="show-off-final-results"></a><a href="#show-off-final-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Show off final results</h3>
<p>Yeah, we have indexed all our images and texts in AquilaDB. Now it's time to retrieve them either by text search or by image search.</p>
<h4><a class="anchor" aria-hidden="true" id="search-images-by-text"></a><a href="#search-images-by-text" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>search images by text</h4>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> json 

<span class="hljs-comment"># search by text</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">search_by_text</span><span class="hljs-params">(text_in)</span>:</span>
    <span class="hljs-comment"># load saved model</span>
    fasttext_model = ft.load_model(<span class="hljs-string">"ftxt_model.bin"</span>)
    <span class="hljs-comment"># generate embeddings</span>
    text_embedding_ = fasttext_model.get_sentence_vector(text_in)
    <span class="hljs-comment"># pad text embedding</span>
    text_embedding_ = np.pad([text_embedding_], ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">1280</span>)), <span class="hljs-string">'constant'</span>)

    <span class="hljs-comment"># convert query matrix</span>
    q_matrix = db.convertMatrix(np.asarray(text_embedding_[<span class="hljs-number">0</span>]))
    <span class="hljs-comment"># do k-NN search</span>
    k = <span class="hljs-number">10</span>
    result = db.getNearest(q_matrix, k)
    <span class="hljs-keyword">return</span> json.loads(result.documents)

<span class="hljs-comment"># render images</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">render_images</span><span class="hljs-params">(doclist)</span>:</span>
    <span class="hljs-keyword">for</span> doc <span class="hljs-keyword">in</span> doclist:
        filename = doc[<span class="hljs-string">"doc"</span>][<span class="hljs-string">"image_name"</span>]
        im = Image.open(<span class="hljs-string">'./flickr30k_images/flickr30k_images/'</span> + filename)
        fig = plt.figure()
        plt.imshow(im)
</code></pre>
<h4><a class="anchor" aria-hidden="true" id="text-to-image-search-1"></a><a href="#text-to-image-search-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>text to image search 1</h4>
<pre><code class="hljs css language-python">render_images(search_by_text(<span class="hljs-string">'people sitting on bench'</span>))
</code></pre>
<p>​</p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862205-09f3fa00-bd23-11e9-9df8-cf77190a3170.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862206-09f3fa00-bd23-11e9-8a00-8401aa2439c8.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862207-0a8c9080-bd23-11e9-9597-9d2f9e114f7c.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862209-0a8c9080-bd23-11e9-9bf1-82b4ff26abda.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862210-0b252700-bd23-11e9-9cb8-a022130fd20b.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862212-0b252700-bd23-11e9-81d6-4005dfde7f27.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862213-0b252700-bd23-11e9-8c1b-ad99dee2c529.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862215-0bbdbd80-bd23-11e9-8b3e-14dd742e222e.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862216-0bbdbd80-bd23-11e9-9b02-0160d2430f76.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862217-0bbdbd80-bd23-11e9-9248-49cc33ae2060.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="text-to-image-search-2"></a><a href="#text-to-image-search-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>text to image search 2</h4>
<pre><code class="hljs css language-python">render_images(search_by_text(<span class="hljs-string">'kids playing in garden'</span>))
</code></pre>
<p>​</p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862218-0c565400-bd23-11e9-87f6-b88bf9d2d5c8.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862221-0c565400-bd23-11e9-9d20-9728f05e843c.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862222-0ceeea80-bd23-11e9-8389-16cb88fbc345.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862223-0ceeea80-bd23-11e9-884b-4eda05d2d1b3.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862224-0ceeea80-bd23-11e9-920c-6d0da4df4f4d.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862225-0d878100-bd23-11e9-9c44-0178b4695f38.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862226-0d878100-bd23-11e9-9797-b62dc6d4bc5e.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862227-0d878100-bd23-11e9-88d8-08bd8e52d83c.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862228-0e201780-bd23-11e9-919a-5453dc435dda.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862229-0e201780-bd23-11e9-926c-9cf3ef4748b0.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="text-to-image-search-3"></a><a href="#text-to-image-search-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>text to image search 3</h4>
<pre><code class="hljs css language-python">render_images(search_by_text(<span class="hljs-string">'man riding a bike'</span>))
</code></pre>
<p>​</p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862230-0e201780-bd23-11e9-8907-8a640771c409.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862233-0eb8ae00-bd23-11e9-90b0-3fd1cdfe8f85.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862234-0eb8ae00-bd23-11e9-8a65-2c9a13037375.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862236-0eb8ae00-bd23-11e9-9d90-24b90c37afad.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862237-0f514480-bd23-11e9-9383-ef404f98a3c3.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862238-0f514480-bd23-11e9-83ee-9b0ab49b7b66.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862241-0fe9db00-bd23-11e9-8536-853b69953d1a.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862242-0fe9db00-bd23-11e9-8ac0-863d4c23f725.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862243-0fe9db00-bd23-11e9-918b-bfa2adefd25a.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862244-10827180-bd23-11e9-91c9-dc793e9e8a1e.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="search-images-by-image"></a><a href="#search-images-by-image" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>search images by image</h4>
<pre><code class="hljs css language-python"><span class="hljs-comment"># search by image</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">search_by_image</span><span class="hljs-params">(image_in, w, h, module)</span>:</span>
    <span class="hljs-comment"># load image</span>
    q_image = load_rsize_image(<span class="hljs-string">'./flickr30k_images/flickr30k_images/'</span> + image_in, w, h)
    q_image = np.array([np.asarray(q_image)])
    <span class="hljs-comment"># generate encodings</span>
    image_encoding_ = get_image_encodings(np.true_divide(q_image, <span class="hljs-number">255</span>), module)
    <span class="hljs-comment"># pad image encodings</span>
    image_encoding_ = np.pad(image_encoding_, ((<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">100</span>, <span class="hljs-number">0</span>)), <span class="hljs-string">'constant'</span>)

    <span class="hljs-comment"># convert query matrix</span>
    q_matrix = db.convertMatrix(np.asarray(image_encoding_[<span class="hljs-number">0</span>]))
    <span class="hljs-comment"># do k-NN search</span>
    k = <span class="hljs-number">10</span>
    result = db.getNearest(q_matrix, k)
    <span class="hljs-keyword">return</span> json.loads(result.documents)
</code></pre>
<h4><a class="anchor" aria-hidden="true" id="image-to-image-search-1"></a><a href="#image-to-image-search-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>image to image search 1</h4>
<pre><code class="hljs css language-python">q_im_file = <span class="hljs-string">'134206.jpg'</span>

<span class="hljs-comment"># show query image</span>
render_images([{<span class="hljs-string">"doc"</span>:{<span class="hljs-string">"image_name"</span>: q_im_file}}])
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862245-10827180-bd23-11e9-8d3b-8d962dd31c35.png" alt="png"></p>
<pre><code class="hljs css language-python"><span class="hljs-comment"># do search</span>
render_images(search_by_image(q_im_file, im_width, im_height, module))
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862246-10827180-bd23-11e9-8f2c-5564cb5f30c2.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862247-111b0800-bd23-11e9-8046-575126d22569.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862249-111b0800-bd23-11e9-8455-aeb597c193ee.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862250-111b0800-bd23-11e9-9fce-88a34a343e25.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862251-11b39e80-bd23-11e9-93e1-c73ecd2ede95.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862252-11b39e80-bd23-11e9-979e-efd85cfb7584.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862253-124c3500-bd23-11e9-8cf9-acfd579cd190.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862255-124c3500-bd23-11e9-8eb5-d6349f6c12f7.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862258-124c3500-bd23-11e9-94f9-5a979cd0fd43.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862259-12e4cb80-bd23-11e9-8d1c-9393b0703ec5.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="image-to-image-search-2"></a><a href="#image-to-image-search-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>image to image search 2</h4>
<pre><code class="hljs css language-python">q_im_file = <span class="hljs-string">'11808546.jpg'</span>

<span class="hljs-comment"># show query image</span>
render_images([{<span class="hljs-string">"doc"</span>:{<span class="hljs-string">"image_name"</span>: q_im_file}}])
<span class="hljs-comment"># do search</span>
render_images(search_by_image(q_im_file, im_width, im_height, module))
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862260-12e4cb80-bd23-11e9-8f66-ea5ca921d5bb.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862261-12e4cb80-bd23-11e9-841e-852ee76a9373.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862263-137d6200-bd23-11e9-8748-bce914750868.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862264-137d6200-bd23-11e9-8139-d7ae6e3b9193.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862265-137d6200-bd23-11e9-91b5-728c7c4ae821.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862266-1415f880-bd23-11e9-8972-eb976ef0126b.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862268-1415f880-bd23-11e9-9f81-35aa1a5e796e.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862270-14ae8f00-bd23-11e9-8f30-42751208e888.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862271-14ae8f00-bd23-11e9-835f-2deed4f7abfd.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862272-14ae8f00-bd23-11e9-86ad-c0d68344f96d.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862273-15472580-bd23-11e9-9ddc-6a006b4871a4.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="image-to-image-search-3"></a><a href="#image-to-image-search-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>image to image search 3</h4>
<pre><code class="hljs css language-python">q_im_file = <span class="hljs-string">'14526359.jpg'</span>

<span class="hljs-comment"># show query image</span>
render_images([{<span class="hljs-string">"doc"</span>:{<span class="hljs-string">"image_name"</span>: q_im_file}}])
<span class="hljs-comment"># do search</span>
render_images(search_by_image(q_im_file, im_width, im_height, module))
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862275-15472580-bd23-11e9-9e82-d13f7882d4ba.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862276-15472580-bd23-11e9-9589-a00c56283334.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862277-15dfbc00-bd23-11e9-9721-46d745c77464.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862278-15dfbc00-bd23-11e9-9938-56bbcd2b9fc0.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862280-15dfbc00-bd23-11e9-9041-8ab6787a2c87.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862281-16785280-bd23-11e9-938d-8dcc81a02d72.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862283-16785280-bd23-11e9-9afc-de85252102a1.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862285-1710e900-bd23-11e9-831e-a159ad44e9e2.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862286-1710e900-bd23-11e9-9fb8-c4c9c8a4a44d.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862287-1710e900-bd23-11e9-9613-3234822685a8.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862288-17a97f80-bd23-11e9-8e55-f8a6886ad77b.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="image-to-image-search-4"></a><a href="#image-to-image-search-4" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>image to image search 4</h4>
<pre><code class="hljs css language-python">q_im_file = <span class="hljs-string">'21164875.jpg'</span>

<span class="hljs-comment"># show query image</span>
render_images([{<span class="hljs-string">"doc"</span>:{<span class="hljs-string">"image_name"</span>: q_im_file}}])
<span class="hljs-comment"># do search</span>
render_images(search_by_image(q_im_file, im_width, im_height, module))
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862289-17a97f80-bd23-11e9-8751-9fe27309d48d.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862291-17a97f80-bd23-11e9-810d-97c001dfa980.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862292-18421600-bd23-11e9-9201-8491cdeffd5e.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862293-18421600-bd23-11e9-9cab-6e93122ccda9.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862294-18421600-bd23-11e9-99b6-4df07b3d423f.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862295-18daac80-bd23-11e9-987a-2212cd2dce59.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862296-19734300-bd23-11e9-8b65-920890a646fb.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862297-19734300-bd23-11e9-90e5-94afb0c0942f.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862298-19734300-bd23-11e9-8463-de29c3df0edc.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862300-1a0bd980-bd23-11e9-80df-c230c2109f34.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862301-1a0bd980-bd23-11e9-8c4f-47beaae6117a.png" alt="png"></p>
<h4><a class="anchor" aria-hidden="true" id="image-to-image-search-5"></a><a href="#image-to-image-search-5" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>image to image search 5</h4>
<pre><code class="hljs css language-python">q_im_file = <span class="hljs-string">'23008340.jpg'</span>

<span class="hljs-comment"># show query image</span>
render_images([{<span class="hljs-string">"doc"</span>:{<span class="hljs-string">"image_name"</span>: q_im_file}}])
<span class="hljs-comment"># do search</span>
render_images(search_by_image(q_im_file, im_width, im_height, module))
</code></pre>
<p><img src="https://user-images.githubusercontent.com/19545678/62862302-1aa47000-bd23-11e9-9ee5-c90c6c4407ea.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862303-1aa47000-bd23-11e9-9ddb-9b0c5dd329cf.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862304-1b3d0680-bd23-11e9-80c5-cf0faf9a5ff8.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862305-1b3d0680-bd23-11e9-88cb-467245055d75.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862306-1b3d0680-bd23-11e9-9353-70d95aecfb4d.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862307-1bd59d00-bd23-11e9-8219-4519b1205736.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862308-1bd59d00-bd23-11e9-9063-cd7de96ed952.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862309-1bd59d00-bd23-11e9-85ba-d8d2b090051e.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862311-1c6e3380-bd23-11e9-9094-320a1de2beb8.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862312-1c6e3380-bd23-11e9-83ed-87fe7b640c59.png" alt="png"></p>
<p><img src="https://user-images.githubusercontent.com/19545678/62862313-1c6e3380-bd23-11e9-8d7f-3e4a4bbb9e8b.png" alt="png"></p>
<p>That's all for this tutorial. Thanks, happy hacking..!</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/semantic-text-retrieval"><span class="arrow-prev">← </span><span>Search text by its meaning</span></a><a class="docs-next button" href="/docs/neural-information-retrieval"><span>Neural Information retrieval</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><h5>Docs</h5><a href="/docs/introduction">Getting Started</a><a href="/docs/get-started">Tutorials</a><a href="/docs/api-reference">API Reference</a></div><div><h5>Community</h5><a href="https://discord.gg/5YP7zHS">Project Chat</a></div><div><h5>More</h5><a href="https://medium.com/a-mma" target="_blank">Blog</a><a href="https://github.com/a-mma/AquilaDB" target="_blank">GitHub</a><a class="github-button" href="https://github.com/a-mma/AquilaDB" data-icon="octicon-star" data-count-href="/AquilaDB/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://github.com/a-mma/AquilaDB" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/img/amma.png" alt="a-mma" width="100" height="45"/></a><section class="copyright">Copyright © 2020 a-mma</section></footer></div></body></html>